{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. Установка инструментов для дальнейшего использования"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:31:19.697280Z","iopub.status.busy":"2024-04-14T17:31:19.696922Z","iopub.status.idle":"2024-04-14T17:32:09.945494Z","shell.execute_reply":"2024-04-14T17:32:09.944591Z","shell.execute_reply.started":"2024-04-14T17:31:19.697249Z"},"trusted":true},"outputs":[],"source":["!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n","!pip install gym[atari]==0.19.0\n","!pip install gym[accept-rom-license]\n","!pip install autorom"]},{"cell_type":"markdown","metadata":{"notebookRunGroups":{"groupValue":""}},"source":["# 2. Импорт используемых в работе библиотек"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:32:22.710110Z","iopub.status.busy":"2024-04-14T17:32:22.709416Z","iopub.status.idle":"2024-04-14T17:32:26.379579Z","shell.execute_reply":"2024-04-14T17:32:26.378766Z","shell.execute_reply.started":"2024-04-14T17:32:22.710074Z"},"trusted":true},"outputs":[],"source":["import gym.wrappers\n","import numpy as np\n","from gym.wrappers import AtariPreprocessing, FrameStack\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Создание среды Breakout. Обработка этой среды, получаю четыре последних кадры игры, чтобы понимать динамику игры. И делаю оттенка серого, чтобы моей модели было проще обучаться."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:32:44.281685Z","iopub.status.busy":"2024-04-14T17:32:44.280671Z","iopub.status.idle":"2024-04-14T17:32:44.515157Z","shell.execute_reply":"2024-04-14T17:32:44.514363Z","shell.execute_reply.started":"2024-04-14T17:32:44.281650Z"},"trusted":true},"outputs":[],"source":["env = gym.make(\"BreakoutNoFrameskip-v4\")\n","env = AtariPreprocessing(env, grayscale_newaxis=True, scale_obs=True)\n","env = FrameStack(env, 4)\n","num_actions = env.action_space.n"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Архитектура модели. Использую три слоя сверточных нейронных сетей, и обыкновенный перцептрон."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:32:45.976446Z","iopub.status.busy":"2024-04-14T17:32:45.975824Z","iopub.status.idle":"2024-04-14T17:32:45.984498Z","shell.execute_reply":"2024-04-14T17:32:45.983512Z","shell.execute_reply.started":"2024-04-14T17:32:45.976417Z"},"trusted":true},"outputs":[],"source":["class QNetwork(nn.Module):\n","    def __init__(self):\n","        super(QNetwork, self).__init__()\n","        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        self.fc1 = nn.Linear(7 * 7 * 64, 652)\n","        self.fc2 = nn.Linear(652, num_actions)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = x.reshape(x.size(0), -1)  # Flatten\n","        x = F.relu(self.fc1(x))\n","        return self.fc2(x)"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Выбор гипрепараметров"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:32:47.169628Z","iopub.status.busy":"2024-04-14T17:32:47.168972Z","iopub.status.idle":"2024-04-14T17:32:47.176628Z","shell.execute_reply":"2024-04-14T17:32:47.175526Z","shell.execute_reply.started":"2024-04-14T17:32:47.169594Z"},"trusted":true},"outputs":[],"source":["gamma = 0.99 # Коэффициент дисконтирования\n","\n","# Выбор эпсилон, для жадной стратегии\n","epsilon = 1.0\n","epsilon_max_1 = 1.0\n","epsilon_min_1 = 0.2\n","epsilon_max_2 = epsilon_min_1\n","epsilon_min_2 = 0.1\n","epsilon_max_3 = epsilon_min_2\n","epsilon_min_3 = 0.02\n","\n","epsilon_interval_1 = (epsilon_max_1 - epsilon_min_1)\n","epsilon_interval_2 = (epsilon_max_2 - epsilon_min_2)\n","epsilon_interval_3 = (epsilon_max_3 - epsilon_min_3)\n","\n","# Количество фреймов для исследования\n","epsilon_greedy_frames = 1000000.0\n","\n","# Количество фреймов, при котором будет совершаться рандомное действие\n","epsilon_random_frames = 50000\n","\n","# Размер Replay Buffer'a для хранения состояния игры\n","max_memory_length = 80000\n","\n","# Размер батча, который будет браться из Replay Buffer'a\n","batch_size = 32\n","\n","# Максимальное кол-во шагов за эпизод\n","max_steps_per_episode = 10000\n","\n","# Обучение будет происходить каждые 20 действой\n","update_after_action = 20\n","\n","# Как часто будет обновляться целевая модель\n","update_target_network = 10000\n"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Объявление модели агента и целевой модели, так же использую функцию потерь Хьюбера, и оптимизатор Адама."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:32:48.185236Z","iopub.status.busy":"2024-04-14T17:32:48.184498Z","iopub.status.idle":"2024-04-14T17:32:50.670507Z","shell.execute_reply":"2024-04-14T17:32:50.669542Z","shell.execute_reply.started":"2024-04-14T17:32:48.185205Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Использую GPU\n","model = QNetwork().to(device)\n","# Target model\n","model_target = QNetwork().to(device)\n","# Using MSE loss for stability\n","loss_function = nn.SmoothL1Loss()\n","optimizer = optim.Adam(model.parameters(), lr=0.00025)"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Инициализация Replay Buffer'a и необходоимых переменных"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:32:54.268673Z","iopub.status.busy":"2024-04-14T17:32:54.267639Z","iopub.status.idle":"2024-04-14T17:32:54.273653Z","shell.execute_reply":"2024-04-14T17:32:54.272603Z","shell.execute_reply.started":"2024-04-14T17:32:54.268637Z"},"trusted":true},"outputs":[],"source":["action_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","\n","episode_reward_history = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0"]},{"cell_type":"markdown","metadata":{},"source":["# 8. Цикл обучения"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T17:32:59.339128Z","iopub.status.busy":"2024-04-14T17:32:59.338228Z","iopub.status.idle":"2024-04-15T02:59:48.564208Z","shell.execute_reply":"2024-04-15T02:59:48.563216Z","shell.execute_reply.started":"2024-04-14T17:32:59.339092Z"},"trusted":true},"outputs":[],"source":["while True:\n","    state = np.array(env.reset())\n","    state = state.reshape(84, 84, 4)\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        frame_count += 1\n","        \n","        # Эпсилон жадная стратегия\n","        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n","            # Выбираю рандомное действие\n","            action = np.random.choice(num_actions)\n","        else:\n","            # Предсказываю функцию полезности действия\n","            state_tensor = torch.tensor(state).unsqueeze(0)\n","            state_tensor = state_tensor.permute(0, 3, 1, 2)\n","            state_tensor= state_tensor.to(device)\n","            action_probs = model(state_tensor).to(device)\n","            # Выбираю лучшее действие\n","            action = torch.argmax(action_probs).item()\n","\n","        # Уменьшаю эпсилон\n","        if frame_count < epsilon_greedy_frames:\n","            epsilon -= epsilon_interval_1 / epsilon_greedy_frames\n","            epsilon = max(epsilon, epsilon_min_1)\n","\n","        if frame_count > epsilon_greedy_frames and frame_count < 2 * epsilon_greedy_frames:\n","            epsilon -= epsilon_interval_2 / epsilon_greedy_frames\n","            epsilon = max(epsilon, epsilon_min_2)\n","\n","        if frame_count > 2 * epsilon_greedy_frames:\n","            epsilon -= epsilon_interval_3 / epsilon_greedy_frames\n","            epsilon = max(epsilon, epsilon_min_3)\n","\n","        # Совершаю действие\n","        state_next, reward, done, _ = env.step(action)\n","        state_next = np.array(state_next)\n","        state_next = state_next.reshape(84, 84, 4)\n","        \n","        episode_reward += reward\n","\n","        # Сохраняю собранные данные в буффер\n","        action_history.append(action)\n","        state_history.append(state)\n","        state_next_history.append(state_next)\n","        done_history.append(done)\n","        rewards_history.append(reward)\n","        state = state_next\n","  \n","        # Обновляю сеть каждые 20 фреймов и беру данные из буфера размером 32\n","        if frame_count % update_after_action == 0 and len(done_history) > batch_size:\n","\n","            # Беру рандомные индексы из буффера размером 32\n","            indices = np.random.choice(range(len(done_history)), size=batch_size)\n","          \n","        \n","            # Теперь беру данные из буффера по этим индексам\n","            state_sample = torch.tensor(np.array([state_history[i] for i in indices])).float()\n","            state_next_sample = torch.tensor(np.array([state_next_history[i] for i in indices])).float()\n","            rewards_sample = torch.tensor(np.array([rewards_history[i] for i in indices])).float()\n","            action_sample = torch.tensor(np.array([action_history[i] for i in indices])).long()\n","            done_sample = torch.tensor([float(done_history[i]) for i in indices])\n","\n","            state_next_sample = state_next_sample.permute(0, 3, 1, 2)\n","            state_sample = state_sample.permute(0, 3, 1, 2)\n","            \n","            state_sample = state_sample.to(device)\n","            action_sample = action_sample.to(device)\n","            rewards_sample = rewards_sample.to(device)\n","            state_next_sample = state_next_sample.to(device)\n","            done_sample = done_sample.to(device)\n","\n","            # Создаю обновленные значения Q для выбранных будущих состояний\n","            # Использую целевую функцию для стабильности\n","            future_rewards = model_target(state_next_sample)\n","            # Q_value = reward + discount factor * expected future reward\n","            updated_q_values = rewards_sample + gamma * torch.max(future_rewards, dim=1).values\n","\n","            \n","            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n","\n","            # Создаю маски, чтобы рассчитать потери для обнавленных масок Q значений\n","            masks = F.one_hot(action_sample, num_actions)\n","\n","            # Вычисляем значение\n","            q_values = model(state_sample)\n","\n","            # Применяем маски к значениям Q, чтобы получить значение Q для предпринятого действия\n","            q_action = torch.sum(q_values * masks, dim=1)\n","\n","            # Считаю потерю между новой Q-функцие и старой\n","            loss = loss_function(q_action, updated_q_values)\n","\n","            # Обратное распространение\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        if frame_count % update_target_network == 0:\n","            # Обновляю веса целевой функции\n","            model_target.load_state_dict(model.state_dict())\n","            # Промежуточные выводы\n","            print(f\"running reward: {running_reward:.2f} at episode {episode_count}, frame count {frame_count}, epsilon {epsilon:.3f}, loss {loss.item():.5f}\")\n","\n","        # Очищаем буффер чтобы он не переполнялся\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","    \n","    # Каждые 500 фрейвом высчитываю среднее вознаграждение\n","    episode_reward_history.append(episode_reward)\n","    if len(episode_reward_history) > 500:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    episode_count += 1\n","\n","    # Завершаю игру если срднее вознаграждение больше 100\n","    if running_reward > 100:\n","      print(\"Solved at episode {}!\".format(episode_count))\n","      break"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-15T03:45:01.813136Z","iopub.status.busy":"2024-04-15T03:45:01.812565Z","iopub.status.idle":"2024-04-15T03:45:01.837993Z","shell.execute_reply":"2024-04-15T03:45:01.836953Z","shell.execute_reply.started":"2024-04-15T03:45:01.813084Z"},"trusted":true},"outputs":[],"source":["torch.save(model, '/kaggle/working/model_breakout.pt')"]},{"cell_type":"markdown","metadata":{},"source":["# Делаем видео игры"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model, 'new_model_breakout_150_reward.pt')\n","torch.save(model_target, 'new__target_model_breakout_150_reward.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_env():\n","    env = gym.make(\"BreakoutNoFrameskip-v4\")\n","    env = AtariPreprocessing(env, grayscale_newaxis=True, scale_obs=True)\n","    env = FrameStack(env, 4)\n","    return env"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = make_env()\n","env = gym.wrappers.Monitor(env, \"./vid1\", force=True)\n","\n","observation = np.array(env.reset())\n","observation = observation.reshape(84, 84, 4)\n","\n","info = 0\n","reward_window = []\n","\n","hits = []\n","bltd = 108 #общее кол-во блоков\n","\n","for i_episode in range(1):\n","    reward_window=[] \n","    epsilon = 0  \n","    for t in range(5000):\n","        if epsilon > np.random.rand(1)[0]:\n","            # Выбираем рандомное действие\n","            action = np.random.choice(4)\n","        else:\n","            # Предсказываю функцию полезности действия\n","            state_tensor = torch.tensor(observation).unsqueeze(0)\n","            state_tensor = state_tensor.permute(0, 3, 1, 2)\n","            state_tensor= state_tensor.to(device)\n","            action_probs = model(state_tensor).to(device)\n","            # Выбираю лучшее действие\n","            action = torch.argmax(action_probs).item()\n","        observation, reward, done, info = env.step(action)\n","        observation = np.array(observation)\n","\n","        observation = observation.reshape(84, 84, 4)\n","        hits.append(reward)\n","        reward_window.append(reward)\n","        if len(reward_window) > 500:\n","          del reward_window[:1] \n","        if len(reward_window) == 500 and np.sum(reward_window) == 0:\n","          print(reward_window)\n","          epsilon = 0.01\n","        else:\n","          epsilon = 0.0001\n","\n","        \n","        if done:\n","            print(\"Потерял одну жизнь за {} время\".format(t+1))\n","            print(info)\n","            \n","            bltd = bltd-np.sum(hits)\n","            hits = []\n","            print(\"Было сломано блоков \", bltd)\n","            if info['ale.lives'] == 0:\n","              break\n","\n","            env.reset()\n","env.close()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
